# ================================================================
# GANN QUANT AI - REINFORCEMENT LEARNING CONFIGURATION v3.0
# ================================================================
# Advanced RL for adaptive trading strategies
# ================================================================

version: "3.0.0"

# ================================================================
# RL FRAMEWORK
# ================================================================
framework:
  
  enabled: true
  library: "stable_baselines3"  # stable_baselines3 | ray_rllib | tensorflow_agents
  
  # Primary algorithms
  algorithms:
    - "ppo"      # Proximal Policy Optimization
    - "td3"      # Twin Delayed DDPG
    - "sac"      # Soft Actor-Critic
    - "a2c"      # Advantage Actor-Critic
    
  # Active algorithm
  active: "ppo"

# ================================================================
# PPO (PROXIMAL POLICY OPTIMIZATION)
# ================================================================
ppo:
  
  enabled: true
  priority: 1
  
  # Network architecture
  policy: "MlpPolicy"  # MlpPolicy | CnnPolicy | MultiInputPolicy
  
  # Hyperparameters
  hyperparameters:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    
  # Network structure
  policy_kwargs:
    net_arch:
      - type: "mlp"
        layers: [256, 256, 128]
    activation_fn: "relu"
    
  # Training
  training:
    total_timesteps: 1000000
    log_interval: 10
    save_freq: 10000
    
  paths:
    model: "models/rl/ppo_trading_agent.zip"
    logs: "logs/rl/ppo"

# ================================================================
# TD3 (TWIN DELAYED DDPG)
# ================================================================
td3:
  
  enabled: true
  priority: 2
  
  # Hyperparameters
  hyperparameters:
    learning_rate: 0.001
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 100
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    policy_delay: 2
    target_policy_noise: 0.2
    target_noise_clip: 0.5
    
  # Network
  policy_kwargs:
    net_arch: [400, 300]
    
  paths:
    model: "models/rl/td3_trading_agent.zip"

# ================================================================
# SAC (SOFT ACTOR-CRITIC)
# ================================================================
sac:
  
  enabled: true
  priority: 2
  
  # Hyperparameters
  hyperparameters:
    learning_rate: 0.0003
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 256
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    ent_coef: "auto"
    target_update_interval: 1
    target_entropy: "auto"
    
  # Network
  policy_kwargs:
    net_arch: [256, 256]
    
  paths:
    model: "models/rl/sac_trading_agent.zip"

# ================================================================
# ENVIRONMENT CONFIGURATION
# ================================================================
environment:
  
  # Custom trading environment
  env_type: "custom_trading_env"
  
  # Market simulation
  simulation:
    start_balance: 100000
    transaction_cost: 0.001  # 0.1%
    slippage: 0.0005  # 0.05%
    
  # State space (observation)
  state_space:
    type: "continuous"
    
    # Features included in state
    features:
      # Price data
      - "normalized_price"
      - "returns"
      - "log_returns"
      
      # Technical indicators
      - "rsi"
      - "macd"
      - "bollinger_position"
      
      # Gann features
      - "gann_square_distance"
      - "gann_angle_position"
      - "time_cycle_phase"
      
      # Ehlers features
      - "mama_fama_diff"
      - "fisher_value"
      - "cycle_period"
      
      # Portfolio state
      - "position_size"
      - "unrealized_pnl"
      - "account_balance_normalized"
      - "drawdown_percent"
      
      # Market regime
      - "volatility_regime"
      - "trend_strength"
      - "market_mode"
      
    dimensions: 30  # Total feature dimensions
    
  # Action space
  action_space:
    type: "continuous"  # continuous | discrete | multi_discrete
    
    # Continuous actions
    continuous:
      # Action: [position_size, stop_loss, take_profit]
      dimensions: 3
      
      # Position size: -1 (full short) to +1 (full long)
      position_size:
        low: -1.0
        high: 1.0
        
      # Stop loss: percentage
      stop_loss:
        low: 0.01
        high: 0.05
        
      # Take profit: R:R ratio
      take_profit:
        low: 2.0
        high: 5.0
        
    # Discrete actions (alternative)
    discrete:
      actions:
        - "hold"
        - "buy_small"
        - "buy_medium"
        - "buy_large"
        - "sell_small"
        - "sell_medium"
        - "sell_large"
        - "close_position"
        
  # Reward function
  reward:
    type: "custom"  # profit | sharpe | custom | sortino
    
    # Custom reward components
    components:
      
      # Profit component
      profit:
        enabled: true
        weight: 0.40
        normalization: "returns"
        
      # Risk-adjusted return
      sharpe:
        enabled: protectiontrue
        weight: 0.30
        lookback: 20
        
      # Drawdown penalty
      drawdown:
        enabled: true
        weight: -0.20
        penalty_threshold: 0.10
        
      # Trade frequency penalty (avoid overtrading)
      trade_frequency:
        enabled: true
        weight: -0.05
        ideal_trades_per_day: 2
        
      # Consistency bonus
      consistency:
        enabled: true
        weight: 0.05
        reward_steady_growth: true
        
    # Reward scaling
    scaling:
      method: "normalize"  # normalize | clip | standardize
      clip_range: [-10, 10]
      
  # Episode configuration
  episode:
    max_steps: 1000  # Maximum steps per episode
    done_on_loss: -0.20  # End episode if loss > 20%

# ================================================================
# TRAINING CONFIGURATION
# ================================================================
training:
  
  # Training phases
  phases:
    
    # Pre-training (optional)
    pretrain:
      enabled: false
      episodes: 1000
      imitation_learning: false
      
    # Main training
    main:
      enabled: true
      total_timesteps: 5000000
      
      # Curriculum learning
      curriculum:
        enabled: true
        stages:
          - difficulty: "easy"
            timesteps: 1000000
            market_condition: "trending"
            
          - difficulty: "medium"
            timesteps: 2000000
            market_condition: "mixed"
            
          - difficulty: "hard"
            timesteps: 2000000
            market_condition: "volatile"
            
    # Fine-tuning
    finetune:
      enabled: true
      timesteps: 500000
      learning_rate: 0.0001
      
  # Training data
  data:
    # Historical data range
    start_date: "2019-01-01"
    end_date: "2024-12-31"
    
    # Data split
    train_split: 0.70
    validation_split: 0.15
    test_split: 0.15
    
    # Data augmentation
    augmentation:
      enabled: true
      methods:
        - "noise_injection"
        - "time_warping"
        - "scenario_replay"
        
  # Exploration strategy
  exploration:
    strategy: "epsilon_greedy"  # epsilon_greedy | ornstein_uhlenbeck | normal_noise
    
    # Epsilon decay
    epsilon:
      start: 1.0
      end: 0.01
      decay: 0.995
      
  # Checkpointing
  checkpointing:
    enabled: true
    save_freq: 50000
    keep_best: 5
    
  # Early stopping
  early_stopping:
    enabled: true
    patience: 100000  # timesteps
    metric: "mean_reward"
    threshold: 0.001  # Minimum improvement

# ================================================================
# EVALUATION
# ================================================================
evaluation:
  
  # Evaluation frequency
  eval_freq: 10000  # timesteps
  n_eval_episodes: 10
  
  # Metrics
  metrics:
    - "mean_reward"
    - "mean_episode_length"
    - "success_rate"
    - "sharpe_ratio"
    - "max_drawdown"
    - "win_rate"
    - "profit_factor"
    
  # Out-of-sample testing
  oos_testing:
    enabled: true
    market_conditions:
      - "bull_market"
      - "bear_market"
      - "sideways"
      - "high_volatility"
      
  # Comparison with baseline
  baseline:
    enabled: true
    strategies:
      - "buy_and_hold"
      - "traditional_strategy"
      - "random_agent"

# ================================================================
# MULTI-AGENT LEARNING
# ================================================================
multi_agent:
  
  enabled: false
  
  # Agent specialization
  agents:
    - name: "trend_trader"
      specialization: "trending_markets"
      weight: 0.4
      
    - name: "range_trader"
      specialization: "ranging_markets"
      weight: 0.3
      
    - name: "volatility_trader"
      specialization: "volatile_markets"
      weight: 0.3
      
  # Coordination
  coordination:
    method: "voting"  # voting | hierarchy | auction
    
  # Communication
  communication:
    enabled: true
    share_oneservations: true
    share_rewards: false

# ================================================================
# ADVANCED TECHNIQUES
# ================================================================
advanced:
  
  # Hindsight Experience Replay (HER)
  her:
    enabled: false
    strategy: "future"
    
  # Prioritized Experience Replay
  per:
    enabled: true
    alpha: 0.6
    beta: 0.4
    
  # Curiosity-driven exploration
  curiosity:
    enabled: true
    intrinsic_reward_weight: 0.1
    
  # Meta-learning
  meta_learning:
    enabled: false
    algorithm: "maml"  # maml | reptile
    
  # Transfer learning
  transfer:
    enabled: true
    source_model: null
    freeze_layers: 0

# ================================================================
# SAFETY CONSTRAINTS
# ================================================================
safety:
  
  # Risk constraints
  constraints:
    max_position_size: 0.10
    max_drawdown: 0.20
    max_daily_loss: 0.05
    
  # Safe exploration
  safe_exploration:
    enabled: true
    
    # Conservative policy during training
    conservative_policy:
      enabled: true
      initial_episodes: 10000
      
  # Intervention
  intervention:
    enabled: true
    
    # Human intervention triggers
    triggers:
      - "large_loss"
      - "unusual_behavior"
      - "constraint_violation"
      
  # Rollback mechanism
  rollback:
    enabled: true
    rollback_on_failure: true

# ================================================================
# DEPLOYMENT
# ================================================================
deployment:
  
  # Model selection
  selection:
    method: "best_validation"  # best_validation | latest | ensemble
    
  # Live trading
  live:
    enabled: false
    paper_trading_first: true
    
    # Gradual rollout
    rollout:
      initial_capital: 10000
      increase_threshold: 0.10  # 10% profit
      max_capital: 100000
      
  # Monitoring
  monitoring:
    enabled: true
    
    # Performance degradation
    degradation:
      detect: true
      threshold: -0.05  # 5% performance drop
      action: "alert"  # alert | pause | rollback
      
  # A/B testing
  ab_testing:
    enabled: true
    split: 0.5  # 50/50 split
    duration_days: 30

# ================================================================
# HYPERPARAMETER OPTIMIZATION
# ================================================================
hpo:
  
  enabled: true
  method: "optuna"  # optuna | hyperopt | ray_tune
  
  # Optimization
  optimization:
    n_trials: 100
    timeout: 86400  # 24 hours
    
    # Parameters to optimize
    parameters:
      learning_rate: [0.00001, 0.01]
      gamma: [0.95, 0.999]
      batch_size: [32, 256]
      n_steps: [256, 4096]
      
  # Pruning
  pruning:
    enabled: true
    patience: 20

# ================================================================
# INTEGRATION
# ================================================================
integration:
  
  # With supervised learning
  supervised:
    enabled: true
    pretrain_policy: true
    expert_demonstrations: false
    
  # With Gann/Ehlers/Astro
  classical_signals:
    enabled: true
    use_as_features: true
    use_as_reward_shaping: true
    
  # Ensemble with other models
  ensemble:
    enabled: true
    weight: 0.20  # RL weight in ensemble

# ================================================================
# LOGGING & DEBUGGING
# ================================================================
logging:
  
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "logs/rl/tensorboard"
    
  # Weights & Biases
  wandb:
    enabled: false
    project: "gann-quant-rl"
    
  # Episode logging
  episodes:
    log_frequency: 100
    save_trajectories: true
    
  # Debugging
  debug:
    enabled: false
    verbose: 1  # 0, 1, or 2
    render: false